{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_oI1y6-cY4d"
   },
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/uditgoel/Downloads\n"
     ]
    }
   ],
   "source": [
    "cd ./Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "XIBjsAqjcY4e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "1Huv7ALVcY4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p07JVvoScY4g"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wseHhYGScY4g"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wH9UvbJTcY4h"
   },
   "source": [
    "We'll be quantifying the similarity between pairs of words of a dataset using different methods with the word co-occurrence in the Brown corpus and synset structure of WordNet. Firstly, we will preprocess the dataset to filter out the rare and ambiguous words. Secondly, we will calculate the similarity scores for pairs of words in the filtered dataset using Lin similarity, NPMI and LSA. Lastly, we will quantify how well these methods work by comparing to a human annotated gold-standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ptNKS9CcY4h"
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "V2OlvNAicY4i"
   },
   "source": [
    "\n",
    "\n",
    "The first filtering is based on **document frequencies**  in the Brown corpus, in order to remove rare words. In this homework, **we will be treating the paragraphs of the Brown corpus as our \"documents\"**. \n",
    "\n",
    "Now we calculate document frequencies for each word type, and use this to remove from your word similarity data any word pairs where at least one of the two words has a document frequency of **$< 8$** in this corpus. We will store all the word pair and similarity mappings in your filtered test set in a dictionary called *filtered_gold_standard*.\n",
    "\n",
    "Note: the document frequency of a word denotes the number of documents that contains the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10593,
     "status": "ok",
     "timestamp": 1588139440049,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "szPlY9PIcY4j",
    "outputId": "218560c5-6e80-4a8b-bfaf-5cb46b90f34c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# filtered_gold_standard stores the word pairs and their human-annotated similarity in your filtered test set\n",
    "filtered_gold_standard = {}\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "set1_df = {}\n",
    "\n",
    "##Reading set1 file\n",
    "with open('set1.tab', newline='') as file:\n",
    "    file = csv.reader(file, delimiter='\\t')\n",
    "    next(file) #Missing the first row\n",
    "    for row in file:\n",
    "        set1_df[(row[0],row[1])] = float(row[2])\n",
    "\n",
    "\n",
    "\n",
    "def preprocessmap(word):\n",
    "    if word.isalpha(): #Checking if word is alphabetic or not\n",
    "        word = word.lower() #Converting to lower case\n",
    "        return lemmatize(word)\n",
    "\n",
    "\n",
    "def flatten_lists(lists): #function to flatten collection of lists\n",
    "    final_list=[]\n",
    "    for i in range(len(lists)):\n",
    "        final_list.extend(lists[i])\n",
    "    return final_list\n",
    "\n",
    "def mylamda(row): #Function to ceck if the condition is meeting or not. \n",
    "    d1,d2=0,0\n",
    "    if row[0] in document_frequency:\n",
    "        d1 = document_frequency[row[0]] \n",
    "    if row[1] in document_frequency :\n",
    "        d2 = document_frequency[row[1]]\n",
    "    return (d1 >=  8 and d2 >= 8)\n",
    "\n",
    "document_frequency = {} #Initialising empty dictionary\n",
    "\n",
    "brown_cor = [list(map(preprocessmap,flatten_lists(i))) for i in brown.paras()] #Applying preprocesssing to flattened lists\n",
    "\n",
    "#Removing elements which are not following condition\n",
    "brown_corpus = [list(set(filter(lambda x: x is not None, i))) for i in brown_cor]\n",
    "\n",
    "for i in brown_corpus: #Loop to make document frequency\n",
    "    for j in i:\n",
    "        if j not in document_frequency:\n",
    "            document_frequency[j] = 1\n",
    "        else:\n",
    "            document_frequency[j] += 1\n",
    "\n",
    "filtered_gold_standard = {k:v for (k,v) in set1_df.items() if mylamda(k) == True }\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "print(len(filtered_gold_standard))\n",
    "print(filtered_gold_standard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTt3T9fycY4p"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PCkSP91lcY4q"
   },
   "outputs": [],
   "source": [
    "assert(len(brown_corpus)==15667)\n",
    "assert(len(filtered_gold_standard) > 50 and len(filtered_gold_standard) < 100)\n",
    "assert(filtered_gold_standard[('love', 'sex')] == 6.77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zknFIccAcY40"
   },
   "source": [
    "\n",
    "\n",
    " Here, we apply the second filtering. The second filtering is based on words with highly ambiguous senses and involves using the NLTK interface to WordNet. Here, we will remove any words which do not have a **single primary sense**. We define single primary sense here as either: (a) having only one sense (i.e. only one synset), or (b) where the count (as provided by the WordNet `count()` method for the lemmas associated with a synset) of the most common sense is at least 4 times larger than the next most common sense. Note that a synset can be associated with multiple lemmas. We will only consider the count of your lemma.\n",
    "\n",
    "Note: You should lowercase the lemmas of a synset when matching your word; and if there are multiple lowercased lemmas that match your word, you should sum up the count of all matching lemmas.\n",
    "\n",
    "Additionally, we will remove any words where the primary sense is **not a noun** (this information is also in the synset). \n",
    "\n",
    "Given this definition, we will remove the word pairs from the test set where at least one of the words does not meet the above criteria. When we have applied the two filtering steps, we will store all the word pair and similarity mappings in your filtered test set in a dictionary called *final_gold_standard*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1588139482398,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "W6rdnrOXcY41",
    "outputId": "8d2391db-d5c0-4ea2-ea83-850e79bffd5c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_gold_standard stores the word pairs and their human-annotated similarity in your final filtered test set\n",
    "final_gold_standard = {}\n",
    "word_primarysense = {} #a dictionary of (word, primary_sense) (used for next section); primary_sense is a synset\n",
    " \n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "def count_lemmas(syn, word):\n",
    "    total = 0\n",
    "    for i in syn.lemmas():\n",
    "        if i.name().lower() == word.lower():\n",
    "            return i.count()\n",
    "\n",
    "def find_count_lemma(synset, word):\n",
    "    count = 0\n",
    "    for i in synset.lemmas():\n",
    "        if i.name().lower() == word.lower():\n",
    "            return count\n",
    "    \n",
    "def two_most_common_words(word):\n",
    "    most_common = None\n",
    "    second_common = None \n",
    "    d1 = sorted(wordnet.synsets(word),key=lambda x: count_lemmas(x, word),reverse=True)\n",
    "    most_common = d1[0]\n",
    "    for i in d1 :\n",
    "        \n",
    "        if i.name().lower() != most_common.name().lower():\n",
    "            second_common = i\n",
    "            break\n",
    "    condition1 = (len(d1) == 1)\n",
    "    if most_common is not None and second_common is not None:\n",
    "        condition2 = (count_lemmas(most_common,word) >= 4*count_lemmas(second_common, word))\n",
    "    condition3 = (most_common.pos() == 'n')\n",
    "    if (condition1 or condition2) and condition3:\n",
    "        word_primarysense[word] = most_common\n",
    "    return (condition1 or condition2) and condition3\n",
    "    \n",
    "\n",
    "\n",
    "final_gold_standard = {k:v for (k,v) in filtered_gold_standard.items() if (two_most_common_words(k[0]) and \\\n",
    "                   two_most_common_words(k[1]))}\n",
    "\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "print(len(final_gold_standard))\n",
    "print(final_gold_standard)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWgLNH3LcY45"
   },
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "th1NNkYZcY45"
   },
   "outputs": [],
   "source": [
    "assert(len(final_gold_standard) > 10 and len(final_gold_standard) < 40)\n",
    "assert(final_gold_standard[('professor', 'doctor')] == 6.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnVPXtiLcY5E"
   },
   "source": [
    "## 2. Computing word similiarity with Lin similarity, NPMI and LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSK4x4IBcY5F"
   },
   "source": [
    "\n",
    "\n",
    "Now we will create several dictionaries with similarity scores for pairs of words in your test set derived using the techniques. The first of these is the Lin similarity for your word pairs using the information content of the Brown corpus, which we will calculate using the primary sense for each word derived above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "spSqNWwbcY5G",
    "outputId": "ae435270-a164-462d-82e9-eb65e8a79c7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "semcor_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "lin_similarities = {}\n",
    "\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "\n",
    "for i in final_gold_standard.keys():\n",
    "    synset1 = word_primarysense[i[0]]\n",
    "    synset2 = word_primarysense[i[1]]\n",
    "    similarity = synset1.lin_similarity(synset2, semcor_ic)\n",
    "    lin_similarities[(i[0],i[1])] = similarity\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "\n",
    "print(lin_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDbnoh-qcY5I"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yH_6wsW6cY5J"
   },
   "outputs": [],
   "source": [
    "assert(lin_similarities[('professor', 'doctor')] > 0.5 and lin_similarities[('professor', 'doctor')] < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FXoMudcY5P"
   },
   "source": [
    "\n",
    " Next, we will calculate Normalized PMI (NPMI) for your word pairs using word frequency derived from the Brown.\n",
    "\n",
    "PMI is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "PMI = \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x,y) = \\frac{\\text{Number of paragraphs with the co-occurrence of x and y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x) = \\frac{\\text{Number of paragraphs with the occurrence of x}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y) = \\frac{\\text{Number of paragraphs with the occurrence of y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "with the sum over $i$ ranging over all paragraphs. Note that there are other ways PMI could be formulated.\n",
    "\n",
    "NPMI is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "NPMI = \\frac{PMI}{-log_2(p(x,y))} = \\frac{log_2(p(x)p(y))}{log_2(p(x,y))} - 1\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, when there is no co-occurrence, NPMI is -1. NPMI is normalized between [-1, +1]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1yoPO9BecY5Q",
    "outputId": "8d7588a5-c769-468e-9c3d-7ca5026e6f4b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# NPMI_similarities stores the word pair and NPMI similarity mappings\n",
    "NPMI_similarities = {}\n",
    "\n",
    "total_words = 0\n",
    "for i in brown_corpus:\n",
    "    total_words += len(i)\n",
    "\n",
    "def cocurrence(word1, word2):\n",
    "    total_cocurrence = 0\n",
    "    word1_occurence = 0\n",
    "    word2_occurence = 0\n",
    "    for i in brown_corpus:\n",
    "        if word1.lower() in i :\n",
    "            word1_occurence += 1 #Calculating word1 frequency\n",
    "        if word2.lower() in i:\n",
    "            word2_occurence += 1 #Calculating word2 frequency\n",
    "        if word1.lower() in i and word2.lower() in i:\n",
    "            total_cocurrence += 1 #Calculating co-occurence frequency\n",
    "    co_prob =  total_cocurrence/total_words\n",
    "    prob_word1 = word1_occurence/total_words\n",
    "    prob_word2 = word2_occurence/total_words\n",
    "    if total_cocurrence == 0: #If no co-occurence then return -1 \n",
    "        return -1\n",
    "    else:\n",
    "        pmi = math.log((co_prob/(prob_word1*prob_word2)),2)\n",
    "        npmi = pmi / (- math.log(co_prob,2))\n",
    "        return npmi\n",
    "\n",
    "for i in final_gold_standard.keys():\n",
    "    NPMI_similarities[(i[0],i[1])] = cocurrence(i[0],i[1])\n",
    "# Your answer BEGINS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "\n",
    "print(NPMI_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "1uJbBXzfcY5S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-nhg3YOBcY5S"
   },
   "outputs": [],
   "source": [
    "assert(NPMI_similarities[('professor', 'doctor')] == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hfkyiBkcY5a"
   },
   "source": [
    "\n",
    "Here we'll use singular value decomposition (SVD) to derive similarity scores using the Latent Semantic Analysis (LSA) method. Recall that LSA applies SVD and truncation to get a dense vector representation of a word type. To measure similarity between two words we calculate cosine similarity between the LSA vectors of the words.\n",
    "\n",
    "We'll first build a term-document frequency matrix (as before, a document is a paragraph in Brown corpus). That is, the rows corresponds to words in the vocabulary, and the columns represent the paragraphs/documents in Brown corpus. Each cell records the document frequency of a word (1 if the word appears in the document, 0 otherwise). \n",
    "Given the term-document frequency matrix, we'll use `truncatedSVD` in `sklearn` to produce dense vectors of length k = 500, and then use cosine similarity to produce similarities for the word pairs in the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17315,
     "status": "ok",
     "timestamp": 1588148041794,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "5qg8WfjVcY5b",
    "outputId": "417407a8-5d17-44c7-e81a-906e9335b42f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "# LSA_similarities stores the word pair and LSA similarity mappings\n",
    "LSA_similarities = {}\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "\n",
    "def cos_sim(a, b): #function to calculate cosine similarity\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def get_BOW(text): #This functions gives bag of words\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] =  1\n",
    "    return BOW\n",
    "\n",
    "texts = []\n",
    "for fileid in brown_corpus:\n",
    "    texts.append(get_BOW(fileid))\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "brown_matrix = vectorizer.fit_transform(texts)\n",
    "brown_matrix_transposed = csr_matrix(brown_matrix).transpose()\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "brown_matrix_lowrank = svd.fit_transform(brown_matrix_transposed)\n",
    "    \n",
    "for i in final_gold_standard.keys():\n",
    "    a = brown_matrix_lowrank[vectorizer.vocabulary_[i[0]]]\n",
    "    b = brown_matrix_lowrank[vectorizer.vocabulary_[i[1]]]\n",
    "    LSA_similarities[(i[0],i[1])] = cos_sim(a,b)\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "\n",
    "print(LSA_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLtjcNgIcY5d"
   },
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oZ0sUZbNcY5e"
   },
   "outputs": [],
   "source": [
    "assert(LSA_similarities[('professor', 'doctor')] > 0 and LSA_similarities[('professor', 'doctor')] < 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing word similarity with feedforward language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Here we'll build a n-gram neural language model to learn word embeddings, and compute word similarity based on the word embeddings. As before we will use the Brown corpus as training data for our language model, and the first step is to collect the vocabulary.\n",
    "\n",
    "As before, we'll treat paragraphs in the Brown corpus as our documents. The first 12K paragraphs/documents will serve as our training data, and the rest (3K+ documents) as development data. The first step towards building a language model is to collect the vocabulary, i.e. the set of unique word types in our training data. When collecting the word types, you should lowercase all words, and only keep word types that have a frequency $>= 5$. Store your vocabulary in the _vocab_ object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14058,
     "status": "ok",
     "timestamp": 1588144253884,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "u2HdOXvYcY5k",
    "outputId": "1b84d6be-d0cd-4bf5-f580-619f122b59b6"
   },
   "outputs": [],
   "source": [
    "num_train = 12000\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "training_data = brown.paras()[:num_train]  #Splitting training data\n",
    "development_set = brown.paras()[num_train:] #Spliting development data\n",
    "term_frequency  = {}\n",
    "for i in training_data: \n",
    "    lists  = flatten_lists(i)\n",
    "    for j in lists:\n",
    "        j = j.lower()\n",
    "        if j not in term_frequency: #Building term frequency data\n",
    "            term_frequency[j] = 1\n",
    "        else:\n",
    "            term_frequency[j] += 1\n",
    "for key,value in term_frequency.items():\n",
    "    if value >= 5:\n",
    "        vocab.add(key.lower())\n",
    "        \n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(vocab) > 8000 and len(vocab) < 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we'll be building a trigram neural language model (based on lecture 7, page 20), the next step is to collect trigrams to construct our training data. In a trigram neural language model, for example if we have the trigram _cow eats grass_, the input to the model is the first two terms of a trigram (_cow_ and _eats_), and the language model's aim is to predict the last term of the trigram (_grass_). Your task here is to construct the training and development data for the language model. Just like the previous step, the first 12K paragraphs will serve as our training data, and the remaining 3K+ will be for development. \n",
    "As an example, given the sentence \"_a big fat hungry cow ._\", you should create the following training examples:\n",
    "\n",
    "|input|target|\n",
    "|:---:|:----:|\n",
    "|<i>a</i>, _big_|_fat_|\n",
    "|_big_, _fat_|_hungry_|\n",
    "|_fat_, _hungry_|_cow_|\n",
    "|_hungry_, _cow_|_._|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "# Your answer BEGINS HERE\n",
    "indexes = list(range(0, len(vocab)))\n",
    "vocab_dict =  dict(zip(vocab, indexes))\n",
    "x_train = []\n",
    "y_train = []\n",
    "for k,i in enumerate(training_data): #Makes training data\n",
    "    lists = flatten_lists(i)\n",
    "    for i in ngrams(lists,3):\n",
    "        \n",
    "        if i[0].lower() not in vocab_dict:\n",
    "            train_1 = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            train_1 = vocab_dict[i[0].lower()]\n",
    "        if i[1].lower() not in vocab_dict:\n",
    "            train_2 = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            train_2 = vocab_dict[i[1].lower()]\n",
    "        x_train.append([train_1, train_2])\n",
    "        if i[2].lower() not in vocab_dict:\n",
    "            test = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            test =  vocab_dict[i[2].lower()]\n",
    "        y_train.append(test)\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for k,i in enumerate(development_set): #Makes development set\n",
    "    lists = flatten_lists(i)\n",
    "    \n",
    "    for i in ngrams(lists,3):\n",
    "        if i[0].lower() not in vocab_dict:\n",
    "            train_1 = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            train_1 = vocab_dict[i[0].lower()]\n",
    "        if i[1].lower() not in vocab_dict:\n",
    "            train_2 = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            train_2 = vocab_dict[i[1].lower()]\n",
    "        x_dev.append([train_1, train_2])\n",
    "        if i[2].lower() not in vocab_dict:\n",
    "            test = vocab_dict['<UNK>']\n",
    "        else:\n",
    "            test =  vocab_dict[i[2].lower()]\n",
    "        y_dev.append(test)\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "#Converting lists to np arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)\n",
    "    \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(x_train.shape[0] == y_train.shape[0])\n",
    "assert(x_dev.shape[0] == y_dev.shape[0])\n",
    "assert(x_train.shape[0] > 500000)\n",
    "assert(x_dev.shape[0] > 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Now let's build the trigram neural language model.\n",
    "$x' = e(x_1) \\oplus e(x_2)$\n",
    "\n",
    "$h = \\tanh(W_1 x' + b)$\n",
    "\n",
    "$y = $ softmax$(W_2 h)$\n",
    "\n",
    "where $\\oplus$ is the concatenation operation, $x_1$ and $x_2$ are the input words, $e$ is an embedding function, and $y$ is the target word. \n",
    "Set the dimension of the word embeddings and $h$ to 100, and train your model with 3 epochs with a batch size of 256. \n",
    "\n",
    "After the model is trained, use the word embeddings to compute word similarity in the test set. Store the similarity scores in a dictionary called *lm_similarities*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 77894,
     "status": "ok",
     "timestamp": 1588147602713,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "qlVRe-opcY5n",
    "outputId": "df59619c-9d39-4e47-fb70-5373dd1d06db"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers, Input\n",
    "from keras.utils import to_categorical\n",
    "lm_similarities = {}\n",
    "vocab_size = len(vocab_dict)\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "\n",
    "#Declaring a feed forward neural network model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = vocab_size, output_dim = 100, input_length = 2)) #Embedding layer\n",
    "model.add(layers.Flatten()) #Flattening layer\n",
    "model.add(layers.Dense(100, activation='tanh')) #Adding hidden layer\n",
    "model.add(layers.Dense(vocab_size, activation = 'softmax')) #Final output layer\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=256, validation_data=(x_dev, y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('bread', 'butter'): 0.34102115, ('professor', 'doctor'): 0.21716338, ('student', 'professor'): 0.33623403, ('stock', 'egg'): 0.49702626, ('money', 'cash'): 0.2844231, ('king', 'queen'): 0.2530256, ('bishop', 'rabbi'): 0.25156778, ('football', 'basketball'): 0.38258144, ('football', 'tennis'): 0.35740107, ('alcohol', 'chemistry'): -0.23344013, ('baby', 'mother'): 0.06469177, ('car', 'automobile'): -0.06626839, ('journey', 'voyage'): 0.107774936, ('coast', 'shore'): -0.15673135, ('brother', 'monk'): 0.570878, ('journey', 'car'): -0.07162175, ('coast', 'hill'): 0.17835954, ('forest', 'graveyard'): 0.07754704, ('monk', 'slave'): -0.06459527, ('coast', 'forest'): 0.47430345, ('psychology', 'doctor'): 0.20336024, ('psychology', 'mind'): 0.08726869, ('psychology', 'health'): 0.40560368, ('psychology', 'science'): 0.56492317, ('planet', 'moon'): 0.4251139, ('planet', 'galaxy'): 0.15662263}\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_layer(index=0).get_weights()[0]\n",
    "for k,i in enumerate(final_gold_standard):\n",
    "    if i[0] not in vocab_dict:\n",
    "        embed1 = embeddings[vocab_dict['<UNK>']]\n",
    "    else:\n",
    "        \n",
    "        embed1 = embeddings[vocab_dict[i[0]]]\n",
    "    if i[1] not in vocab_dict:\n",
    "        embed2 = embeddings[vocab_dict[\"<UNK>\"]]\n",
    "    else:\n",
    "        embed2 = embeddings[vocab_dict[i[1]]]\n",
    "    lm_similarities[(i[0],i[1])] = cos_sim(embed1, embed2)\n",
    "print(lm_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTgtO9-pcY5q"
   },
   "source": [
    "## 3. Comparison with the Gold Standard (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UpgjOKmcY5r"
   },
   "source": [
    "\n",
    " Finally, we will compare all the similarities you've created to the gold standard you loaded and filtered in the first step. For this, you can use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1588147762343,
     "user": {
      "displayName": "Jey Han Lau",
      "photoUrl": "",
      "userId": "09065329932778503205"
     },
     "user_tz": -600
    },
    "id": "oAgJwXhxcY5r",
    "outputId": "0c5cc8f1-2aca-4fbc-cb1b-2c7a9529945d"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# pearson_correlations stores the pearson correlations with the gold standard of 'lin', 'NPMI', 'LSA', 'lm'\n",
    "pearson_correlations = {}\n",
    "\n",
    "# Your answer BEGINS HERE\n",
    "\n",
    "def corr(dict1, dict2):\n",
    "    x,y = [], []\n",
    "    for i in dict1.keys():\n",
    "        x.append(dict1[i])\n",
    "        y.append(dict2[i])\n",
    "    return pearsonr(x,y)[0]\n",
    "\n",
    "##Adding keys to dictionary\n",
    "\n",
    "pearson_correlations['lin'] = corr(lin_similarities, final_gold_standard)\n",
    "pearson_correlations['NPMI'] = corr(NPMI_similarities, final_gold_standard)\n",
    "pearson_correlations['LSA'] = corr(LSA_similarities, final_gold_standard)\n",
    "pearson_correlations['lm'] = corr(lm_similarities, final_gold_standard)\n",
    "\n",
    "# Your answer ENDS HERE\n",
    "\n",
    "\n",
    "print(pearson_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aV6PxadqcY5v"
   },
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q2fNHrT0cY5v"
   },
   "outputs": [],
   "source": [
    "assert(pearson_correlations['lin'] > 0.4 and pearson_correlations['lin'] < 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIoExaG0cY51"
   },
   "source": [
    "## A final word\n",
    "\n",
    "Normally, we would not use a corpus as small as the Brown for the purposes of building word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aCwCHklbcY52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "word-similarity.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
